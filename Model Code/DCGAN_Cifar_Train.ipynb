{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13531,
     "status": "ok",
     "timestamp": 1676700436695,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "doTox_zHyb2u"
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from PIL import ImageFilter\n",
    "import textwrap, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1831,
     "status": "ok",
     "timestamp": 1676700486163,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "C8VbX4L9PPPz",
    "outputId": "1453afe6-827f-4ee4-d22b-9bae2ea88f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=TensorSpec(shape=(128, 32, 32, None), dtype=tf.float32, name=None)>\n",
      "<PrefetchDataset element_spec=TensorSpec(shape=(128, 32, 32, None), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "\n",
    "    image = tf.image.resize(image, (32, 32))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = image[..., :3]  # Remove the fourth channel if present\n",
    "\n",
    "    return image\n",
    "\n",
    "def create_dataset(image_paths, batch_size, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    print(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Load image paths\n",
    "image_paths = [path for path in tf.io.gfile.glob(r'C:\\Users\\diego\\Documents\\cifar10\\plane\\*.png')]\n",
    "\n",
    "# Split the image paths into a training set and validation set\n",
    "train_image_paths = image_paths[:int(len(image_paths) * 0.8)]\n",
    "val_image_paths = image_paths[int(len(image_paths) * 0.8):]\n",
    "\n",
    "# Create the training and validation datasets\n",
    "batch_size = 128\n",
    "train_dataset = create_dataset(train_image_paths, batch_size, is_training=True)\n",
    "val_dataset = create_dataset(val_image_paths, batch_size, is_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1676700488062,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "dtNCK1mKPV7T"
   },
   "outputs": [],
   "source": [
    "# Declaration of the hyperparameters\n",
    "\n",
    "# data\n",
    "num_epochs = 500 \n",
    "image_size = 32\n",
    "# resolution of Kernel Inception Distance measurement, see related section\n",
    "kid_image_size = 75\n",
    "padding = 0.25\n",
    "\n",
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "target_accuracy = 0.85\n",
    "integration_steps = 1000\n",
    "\n",
    "# architecture\n",
    "noise_size = 128\n",
    "depth = 4\n",
    "width = 128\n",
    "leaky_relu_slope = 0.2\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# optimization\n",
    "batch_size = 128\n",
    "learning_rate = 2e-3\n",
    "beta_1 = 0.5\n",
    "ema = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1676700500020,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "dbA3S6pKPXIU"
   },
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"kid\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean()\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9415,
     "status": "ok",
     "timestamp": 1676700538732,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "rPbE6IhhPZ_6",
    "outputId": "adf050db-a5e1-4a1b-f767-ae09d66932b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              262144    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             6144      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 2048)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 256)        524288    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 4, 256)        768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 256)        1048576   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 256)        768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 256)      1048576   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 256)      768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 16, 16, 128)      524288    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 16, 128)      384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 32, 32, 3)        6147      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,422,851\n",
      "Trainable params: 3,416,963\n",
      "Non-trainable params: 5,888\n",
      "_________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 128)       6144      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 16, 16, 128)      384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 128)        384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 4, 128)        384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 2, 2, 128)        384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 794,625\n",
      "Trainable params: 793,601\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# \"hard sigmoid\", useful for binary accuracy calculation from logits\n",
    "def step(values):\n",
    "    # negative values -> 0.0, positive values -> 1.0\n",
    "    return 0.5 * (1.0 + tf.sign(values))\n",
    "\n",
    "\n",
    "# augments images with a probability that is dynamically updated during training\n",
    "class AdaptiveAugmenter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # stores the current probability of an image being augmented\n",
    "        self.probability = tf.Variable(0.0)\n",
    "\n",
    "        # the corresponding augmentation names from the paper are shown above each layer\n",
    "        # the authors show, that the blitting and geometric augmentations\n",
    "        # are the most helpful in the low-data regime\n",
    "        self.augmenter = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                # blitting/x-flip:\n",
    "                layers.RandomFlip(\"horizontal\"),\n",
    "                # blitting/integer translation:\n",
    "                layers.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training):\n",
    "        if training:\n",
    "            augmented_images = self.augmenter(images, training)\n",
    "\n",
    "            # during training either the original or the augmented images are selected\n",
    "            # based on self.probability\n",
    "            augmentation_values = tf.random.uniform(\n",
    "                shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "            )\n",
    "            augmentation_bools = tf.math.less(augmentation_values, self.probability)\n",
    "\n",
    "            images = tf.where(augmentation_bools, augmented_images, images)\n",
    "        return images\n",
    "\n",
    "    def update(self, real_logits):\n",
    "        current_accuracy = tf.reduce_mean(step(real_logits))\n",
    "\n",
    "        # the augmentation probability is updated based on the dicriminator's\n",
    "        # accuracy on real images\n",
    "        accuracy_error = current_accuracy - target_accuracy\n",
    "        self.probability.assign(\n",
    "            tf.clip_by_value(\n",
    "                self.probability + accuracy_error / integration_steps, 0.0, 1.0\n",
    "            )\n",
    "        )        \n",
    "\n",
    "# DCGAN generator\n",
    "def get_generator():\n",
    "\n",
    "    # receive noise_input\n",
    "    noise_input = keras.Input(shape=(noise_size,))\n",
    "    # dense neural network\n",
    "    x = layers.Dense(4 * 4 * width, use_bias=False)(noise_input)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # reshape to pass it through the convolutional networks for \"upsampling\" and generating the desired images\n",
    "    x = layers.Reshape(target_shape=(4, 4, width))(x)\n",
    "    # first same convolution to increase number of channels while keeping the size\n",
    "    x = layers.Conv2DTranspose(\n",
    "        width*2, kernel_size=4, strides=1, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)    \n",
    "    # convolutions to increase the size to generate the novel images\n",
    "    x = layers.Conv2DTranspose(\n",
    "        width*2, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)     \n",
    "    x = layers.Conv2DTranspose(\n",
    "        width*2, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x) \n",
    "    # second same convolution to decrease number of channels while keeping the size\n",
    "    x = layers.Conv2DTranspose(\n",
    "        width, kernel_size=4, strides=1, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x) \n",
    "    # last convolution with sigmoid activation to get the image \n",
    "    image_output = layers.Conv2DTranspose(\n",
    "        3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\",\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(noise_input, image_output, name=\"generator\")\n",
    "\n",
    "\n",
    "# DCGAN discriminator\n",
    "def get_discriminator():\n",
    "    image_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    x = image_input\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.LeakyReLU(alpha=leaky_relu_slope)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_score = layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(image_input, output_score, name=\"discriminator\")\n",
    "\n",
    "\n",
    "class GAN_ADA(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmenter = AdaptiveAugmenter()\n",
    "        self.generator = get_generator()\n",
    "        self.ema_generator = keras.models.clone_model(self.generator)\n",
    "        self.discriminator = get_discriminator()\n",
    "\n",
    "        self.generator.summary()\n",
    "        self.discriminator.summary()\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        # separate optimizers for the two networks\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "        self.generator_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.real_accuracy = keras.metrics.BinaryAccuracy(name=\"real_acc\")\n",
    "        self.generated_accuracy = keras.metrics.BinaryAccuracy(name=\"gen_acc\")\n",
    "        self.augmentation_probability_tracker = keras.metrics.Mean(name=\"aug_p\")\n",
    "        self.kid = KID()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.generator_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.real_accuracy,\n",
    "            self.generated_accuracy,\n",
    "            self.augmentation_probability_tracker,\n",
    "            self.kid,\n",
    "        ]\n",
    "\n",
    "    def generate(self, batch_size, training):\n",
    "        latent_samples = tf.random.normal(shape=(batch_size, noise_size))\n",
    "        # use ema_generator during inference\n",
    "        if training:\n",
    "            generated_images = self.generator(latent_samples, training)\n",
    "        else:\n",
    "            generated_images = self.ema_generator(latent_samples, training)\n",
    "        return generated_images\n",
    "\n",
    "    def adversarial_loss(self, real_logits, generated_logits):\n",
    "        # this is usually called the non-saturating GAN loss\n",
    "\n",
    "        real_labels = tf.ones(shape=(batch_size, 1))\n",
    "        generated_labels = tf.zeros(shape=(batch_size, 1))\n",
    "\n",
    "        # the generator tries to produce images that the discriminator considers as real\n",
    "        generator_loss = keras.losses.binary_crossentropy(\n",
    "            real_labels, generated_logits, from_logits=False\n",
    "        )\n",
    "        # the discriminator tries to determine if images are real or generated\n",
    "        discriminator_loss = keras.losses.binary_crossentropy(\n",
    "            tf.concat([real_labels, generated_labels], axis=0),\n",
    "            tf.concat([real_logits, generated_logits], axis=0),\n",
    "            from_logits=False,\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(generator_loss), tf.reduce_mean(discriminator_loss)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        real_images = self.augmenter(real_images, training=True)\n",
    "\n",
    "        # use persistent gradient tape because gradients will be calculated twice\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            generated_images = self.generate(batch_size, training=True)\n",
    "            # gradient is calculated through the image augmentation\n",
    "            generated_images = self.augmenter(generated_images, training=True)\n",
    "\n",
    "            # separate forward passes for the real and generated images, meaning\n",
    "            # that batch normalization is applied separately\n",
    "            real_logits = self.discriminator(real_images, training=True)\n",
    "            generated_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            generator_loss, discriminator_loss = self.adversarial_loss(\n",
    "                real_logits, generated_logits\n",
    "            )\n",
    "\n",
    "        # calculate gradients and update weights\n",
    "        generator_gradients = tape.gradient(\n",
    "            generator_loss, self.generator.trainable_weights\n",
    "        )\n",
    "        discriminator_gradients = tape.gradient(\n",
    "            discriminator_loss, self.discriminator.trainable_weights\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_weights)\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # update the augmentation probability based on the discriminator's performance\n",
    "        self.augmenter.update(real_logits)\n",
    "\n",
    "        self.generator_loss_tracker.update_state(generator_loss)\n",
    "        self.discriminator_loss_tracker.update_state(discriminator_loss)\n",
    "        self.real_accuracy.update_state(1.0, step(real_logits))\n",
    "        self.generated_accuracy.update_state(0.0, step(generated_logits))\n",
    "        self.augmentation_probability_tracker.update_state(self.augmenter.probability)\n",
    "\n",
    "        # track the exponential moving average of the generator's weights to decrease\n",
    "        # variance in the generation quality\n",
    "        for weight, ema_weight in zip(\n",
    "            self.generator.weights, self.ema_generator.weights\n",
    "        ):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    def test_step(self, real_images):\n",
    "        generated_images = self.generate(batch_size, training=False)\n",
    "\n",
    "        self.kid.update_state(real_images, generated_images)\n",
    "\n",
    "        # only KID is measured during the evaluation phase for computational efficiency\n",
    "        return {self.kid.name: self.kid.result()}\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6, interval=5):\n",
    "        # plot random generated images for visual evaluation of generation quality\n",
    "        if epoch is None or (epoch + 1) % interval == 0:\n",
    "            num_images = num_rows * num_cols\n",
    "            generated_images = self.generate(num_images, training=False)\n",
    "\n",
    "            plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    index = row * num_cols + col\n",
    "                    plt.subplot(num_rows, num_cols, index + 1)\n",
    "                    plt.imshow(generated_images[index])\n",
    "                    plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "# create and compile the model\n",
    "model = GAN_ADA()\n",
    "model.compile(\n",
    "    generator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    "    discriminator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    ")\n",
    "\n",
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"gan_model_ll\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giQNDVVSPjk0",
    "outputId": "75dbac3b-b5c1-4167-a101-6a59e06cd382"
   },
   "outputs": [],
   "source": [
    "# run training and plot generated images periodically\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAvuhJs0P7-h"
   },
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "tf.saved_model.save(model, \"DCGAN_Cifar_Plane\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvaaIxehQxP9"
   },
   "outputs": [],
   "source": [
    "def plot_images_best(model, num_rows=5, num_cols=4):\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    \n",
    "    # Generate a large number of images, to choose the best from\n",
    "    num_images = 5000\n",
    "    generated_images = model.generate(num_images, training=False)\n",
    "\n",
    "    # Extract the indices of the n best generated images based on the output from the discriminator\n",
    "    scores = model.discriminator(generated_images)\n",
    "    num_gen = num_rows * num_cols\n",
    "    v, ind = tf.math.top_k(scores[:, 0], k=num_gen, sorted=False)\n",
    "    print(v)\n",
    "    print(ind)\n",
    "    \n",
    "    # Iterate over the best images and plot them all\n",
    "    plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            index = row * num_cols + col\n",
    "            ii = ind[index]\n",
    "            plt.subplot(num_rows, num_cols, index + 1)\n",
    "            plt.imshow(generated_images[ii])\n",
    "            \n",
    "            # Convert the tensor to a PIL Image\n",
    "            pil_img = tf.keras.preprocessing.image.array_to_img(generated_images[ii])\n",
    "\n",
    "            # Save the image to a file\n",
    "            pil_img.save(\"generated_images_cifar_plane_\"+str(index)+\".png\")\n",
    "            \n",
    "            plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()       \n",
    "        \n",
    "plot_images_best(model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_sSslWnJ2O4"
   },
   "outputs": [],
   "source": [
    "# Load ESRGAN model to increase the size of the generated images and improve their resolution \n",
    "\n",
    "os.environ[\"TFHUB_DOWNLOAD_PROGRESS\"] = \"True\"\n",
    "# Declaring Constants\n",
    "ESRGAN_path = \"https://tfhub.dev/captain-pool/esrgan-tf2/1\"\n",
    "model_super = hub.load(ESRGAN_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MQohvs6J6VS"
   },
   "outputs": [],
   "source": [
    "# Post Processing Functions to increase the size and resolution of the generated images\n",
    "\n",
    "def postprocess(image_tensor):\n",
    "    \n",
    "\n",
    "    size = image_tensor.shape[1]\n",
    "    new_size = size * 2\n",
    "    # Upsample the image using TensorFlow\n",
    "    resized_image = tf.image.resize(image_tensor, [new_size, new_size], method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "    # Apply denoising using OpenCV\n",
    "    denoised_image = cv2.medianBlur(resized_image.numpy(), 3)\n",
    "\n",
    "    #Applying sharpening filter\n",
    "    pil_img = tf.keras.preprocessing.image.array_to_img(denoised_image)\n",
    "    sharp = pil_img.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    return sharp\n",
    "\n",
    "def resize_res(image_tensor):\n",
    "\n",
    "    # First denoise the image and prepare it for the ESRGAN model \n",
    "    denoised_image = cv2.medianBlur(image_tensor.numpy(), 3)\n",
    "    image = tf.expand_dims(denoised_image, 0)\n",
    "    image = image * 255\n",
    "\n",
    "    # Improve the image with the ESRGAN model\n",
    "    fake_image = model_super(image,training=False)\n",
    "\n",
    "    # Prepare image for plotting and later use\n",
    "    fake_image = tf.squeeze(fake_image)\n",
    "    fake_image = fake_image / 255\n",
    "\n",
    "    return fake_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aGqpKHxKGzC"
   },
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "\n",
    "    # Declare necessary variables for plotting configuration\n",
    "    columns=5\n",
    "    width=25\n",
    "    height=10\n",
    "\n",
    "    # Check if there are images for plotting\n",
    "    if not images:\n",
    "        print(\"No images to display.\")\n",
    "        return \n",
    "\n",
    "    # Plotting configuration    \n",
    "    height = max(height, int(len(images)/columns) * height)\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    # Plot the images in subplots\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0Qy5lgqKNqP"
   },
   "outputs": [],
   "source": [
    "# Function to plot the best images generated from the model and their resized versions\n",
    "\n",
    "def plot_images_post(model, num_generated_images):\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    \n",
    "    # Generate a large number of images, to choose the best from\n",
    "    num_images = 5000\n",
    "    generated_images = model.generate(num_images, training=False)\n",
    "\n",
    "    # Extract the indices of the n best generated images based on the output from the discriminator\n",
    "    scores = model.discriminator(generated_images)\n",
    "    v, ind = tf.math.top_k(scores[:, 0], k=num_generated_images, sorted=True)\n",
    "    print(v)\n",
    "    print(ind)\n",
    "\n",
    "    # Iterate over the best images, apply the resizing and improvement functions and plot them all\n",
    "    for i in range(len(ind)):\n",
    "\n",
    "        # Extract the best images based on the indices\n",
    "        ii = ind[i]\n",
    "        image_tensor = generated_images[ii]\n",
    "\n",
    "        # First resizing and sharpening of the image (output size = 64*64*3)\n",
    "        sharp = postprocess(image_tensor)\n",
    "\n",
    "        # Second resizing and sharpening of the image (output size = 128*128*3)\n",
    "        sharp_array = tf.keras.preprocessing.image.img_to_array(sharp)\n",
    "        sharp1 = postprocess(sharp_array)\n",
    "\n",
    "        # Improvement of the resolution and resizing using the ESRGAN model (output size = 128*128*3)\n",
    "        res_image = resize_res(image_tensor)\n",
    "\n",
    "        # Second improvement of the resolution and resizing using the ESRGAN model (output size = 256*256*3)\n",
    "        resized_image_2 = tf.image.resize(image_tensor, [64, 64], method=tf.image.ResizeMethod.BICUBIC)\n",
    "        res_image_256 = resize_res(resized_image_2)\n",
    "\n",
    "        # Plotting of all images\n",
    "        imgs = [image_tensor,sharp,sharp1,res_image,res_image_256]\n",
    "        display_images(imgs)\n",
    "    \n",
    "num_generated_images = 20        \n",
    "plot_images_post(model,num_generated_images)        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuhNO53aWXYkO+cYDxG14R",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
