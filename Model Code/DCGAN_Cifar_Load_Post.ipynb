{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxrKWgnOB2m4"
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from PIL import ImageFilter\n",
    "import textwrap, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIgnnQwwCCGJ"
   },
   "outputs": [],
   "source": [
    "# Declaration of the hyperparameters\n",
    "\n",
    "# data\n",
    "num_epochs = 500 \n",
    "image_size = 32\n",
    "# resolution of Kernel Inception Distance measurement, see related section\n",
    "kid_image_size = 75\n",
    "padding = 0.25\n",
    "\n",
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "target_accuracy = 0.85\n",
    "integration_steps = 1000\n",
    "\n",
    "# architecture\n",
    "noise_size = 128\n",
    "depth = 4\n",
    "width = 128\n",
    "leaky_relu_slope = 0.2\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# optimization\n",
    "batch_size = 128\n",
    "learning_rate = 2e-3\n",
    "beta_1 = 0.5\n",
    "ema = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGcZURjiCFCF"
   },
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"kid\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean()\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5710,
     "status": "ok",
     "timestamp": 1676694857407,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "gCz8Qk8ECHYD",
    "outputId": "da5289db-d563-4826-8dd3-59b92d05a812"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              262144    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             6144      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 2048)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 256)        524288    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 4, 256)        768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 256)        1048576   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 256)        768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 256)      1048576   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 256)      768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 16, 16, 128)      524288    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 16, 128)      384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 32, 32, 3)        6147      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,422,851\n",
      "Trainable params: 3,416,963\n",
      "Non-trainable params: 5,888\n",
      "_________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 128)       6144      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 16, 16, 128)      384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 128)        384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 4, 128)        384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 128)         262144    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 2, 2, 128)        384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 794,625\n",
      "Trainable params: 793,601\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# \"hard sigmoid\", useful for binary accuracy calculation from logits\n",
    "def step(values):\n",
    "    # negative values -> 0.0, positive values -> 1.0\n",
    "    return 0.5 * (1.0 + tf.sign(values))\n",
    "\n",
    "\n",
    "# augments images with a probability that is dynamically updated during training\n",
    "class AdaptiveAugmenter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # stores the current probability of an image being augmented\n",
    "        self.probability = tf.Variable(0.0)\n",
    "\n",
    "        # the corresponding augmentation names from the paper are shown above each layer\n",
    "        # the authors show, that the blitting and geometric augmentations\n",
    "        # are the most helpful in the low-data regime\n",
    "        self.augmenter = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                # blitting/x-flip:\n",
    "                layers.RandomFlip(\"horizontal\"),\n",
    "                # blitting/integer translation:\n",
    "                layers.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training):\n",
    "        if training:\n",
    "            augmented_images = self.augmenter(images, training)\n",
    "\n",
    "            # during training either the original or the augmented images are selected\n",
    "            # based on self.probability\n",
    "            augmentation_values = tf.random.uniform(\n",
    "                shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "            )\n",
    "            augmentation_bools = tf.math.less(augmentation_values, self.probability)\n",
    "\n",
    "            images = tf.where(augmentation_bools, augmented_images, images)\n",
    "        return images\n",
    "\n",
    "    def update(self, real_logits):\n",
    "        current_accuracy = tf.reduce_mean(step(real_logits))\n",
    "\n",
    "        # the augmentation probability is updated based on the dicriminator's\n",
    "        # accuracy on real images\n",
    "        accuracy_error = current_accuracy - target_accuracy\n",
    "        self.probability.assign(\n",
    "            tf.clip_by_value(\n",
    "                self.probability + accuracy_error / integration_steps, 0.0, 1.0\n",
    "            )\n",
    "        )        \n",
    "\n",
    "# DCGAN generator\n",
    "def get_generator():\n",
    "\n",
    "    # receive noise_input\n",
    "    noise_input = keras.Input(shape=(noise_size,))\n",
    "    # dense neural network\n",
    "    x = layers.Dense(4 * 4 * width, use_bias=False)(noise_input)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # reshape to pass it through the convolutional networks for \"upsampling\" and generating the desired images\n",
    "    x = layers.Reshape(target_shape=(4, 4, width))(x)\n",
    "    # first same convolution to increase number of channels while keeping the size\n",
    "    x = layers.Conv2DTranspose(\n",
    "        width*2, kernel_size=4, strides=1, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)    \n",
    "    # convolutions to increase the size to generate the novel images\n",
    "    x = layers.Conv2DTranspose(\n",
    "        width*2, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)     \n",
    "    x = layers.Conv2DTranspose(\n",
    "        width*2, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x) \n",
    "    # second same convolution to decrease number of channels while keeping the size\n",
    "    x = layers.Conv2DTranspose(\n",
    "        width, kernel_size=4, strides=1, padding=\"same\", use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x) \n",
    "    # last convolution with sigmoid activation to get the image \n",
    "    image_output = layers.Conv2DTranspose(\n",
    "        3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\",\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(noise_input, image_output, name=\"generator\")\n",
    "\n",
    "\n",
    "# DCGAN discriminator\n",
    "def get_discriminator():\n",
    "    image_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    x = image_input\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.LeakyReLU(alpha=leaky_relu_slope)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_score = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "\n",
    "    return keras.Model(image_input, output_score, name=\"discriminator\")\n",
    "\n",
    "\n",
    "class GAN_ADA(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmenter = AdaptiveAugmenter()\n",
    "        self.generator = get_generator()\n",
    "        self.ema_generator = keras.models.clone_model(self.generator)\n",
    "        self.discriminator = get_discriminator()\n",
    "\n",
    "        self.generator.summary()\n",
    "        self.discriminator.summary()\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        # separate optimizers for the two networks\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "        self.generator_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.real_accuracy = keras.metrics.BinaryAccuracy(name=\"real_acc\")\n",
    "        self.generated_accuracy = keras.metrics.BinaryAccuracy(name=\"gen_acc\")\n",
    "        self.augmentation_probability_tracker = keras.metrics.Mean(name=\"aug_p\")\n",
    "        self.kid = KID()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.generator_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.real_accuracy,\n",
    "            self.generated_accuracy,\n",
    "            self.augmentation_probability_tracker,\n",
    "            self.kid,\n",
    "        ]\n",
    "\n",
    "    def generate(self, batch_size, training):\n",
    "        latent_samples = tf.random.normal(shape=(batch_size, noise_size))\n",
    "        # use ema_generator during inference\n",
    "        if training:\n",
    "            generated_images = self.generator(latent_samples, training)\n",
    "        else:\n",
    "            generated_images = self.ema_generator(latent_samples, training)\n",
    "        return generated_images\n",
    "\n",
    "    def adversarial_loss(self, real_logits, generated_logits):\n",
    "        # this is usually called the non-saturating GAN loss\n",
    "\n",
    "        real_labels = tf.ones(shape=(batch_size, 1))\n",
    "        generated_labels = tf.zeros(shape=(batch_size, 1))\n",
    "\n",
    "        # the generator tries to produce images that the discriminator considers as real\n",
    "        generator_loss = keras.losses.binary_crossentropy(\n",
    "            real_labels, generated_logits, from_logits=True\n",
    "        )\n",
    "        # the discriminator tries to determine if images are real or generated\n",
    "        discriminator_loss = keras.losses.binary_crossentropy(\n",
    "            tf.concat([real_labels, generated_labels], axis=0),\n",
    "            tf.concat([real_logits, generated_logits], axis=0),\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(generator_loss), tf.reduce_mean(discriminator_loss)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        real_images = self.augmenter(real_images, training=True)\n",
    "\n",
    "        # use persistent gradient tape because gradients will be calculated twice\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            generated_images = self.generate(batch_size, training=True)\n",
    "            # gradient is calculated through the image augmentation\n",
    "            generated_images = self.augmenter(generated_images, training=True)\n",
    "\n",
    "            # separate forward passes for the real and generated images, meaning\n",
    "            # that batch normalization is applied separately\n",
    "            real_logits = self.discriminator(real_images, training=True)\n",
    "            generated_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            generator_loss, discriminator_loss = self.adversarial_loss(\n",
    "                real_logits, generated_logits\n",
    "            )\n",
    "\n",
    "        # calculate gradients and update weights\n",
    "        generator_gradients = tape.gradient(\n",
    "            generator_loss, self.generator.trainable_weights\n",
    "        )\n",
    "        discriminator_gradients = tape.gradient(\n",
    "            discriminator_loss, self.discriminator.trainable_weights\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_weights)\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # update the augmentation probability based on the discriminator's performance\n",
    "        self.augmenter.update(real_logits)\n",
    "\n",
    "        self.generator_loss_tracker.update_state(generator_loss)\n",
    "        self.discriminator_loss_tracker.update_state(discriminator_loss)\n",
    "        self.real_accuracy.update_state(1.0, step(real_logits))\n",
    "        self.generated_accuracy.update_state(0.0, step(generated_logits))\n",
    "        self.augmentation_probability_tracker.update_state(self.augmenter.probability)\n",
    "\n",
    "        # track the exponential moving average of the generator's weights to decrease\n",
    "        # variance in the generation quality\n",
    "        for weight, ema_weight in zip(\n",
    "            self.generator.weights, self.ema_generator.weights\n",
    "        ):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    def test_step(self, real_images):\n",
    "        generated_images = self.generate(batch_size, training=False)\n",
    "\n",
    "        self.kid.update_state(real_images, generated_images)\n",
    "\n",
    "        # only KID is measured during the evaluation phase for computational efficiency\n",
    "        return {self.kid.name: self.kid.result()}\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6, interval=5):\n",
    "        # plot random generated images for visual evaluation of generation quality\n",
    "        if epoch is None or (epoch + 1) % interval == 0:\n",
    "            num_images = num_rows * num_cols\n",
    "            generated_images = self.generate(num_images, training=False)\n",
    "\n",
    "            plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    index = row * num_cols + col\n",
    "                    plt.subplot(num_rows, num_cols, index + 1)\n",
    "                    plt.imshow(generated_images[index])\n",
    "                    plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "# create and compile the model\n",
    "model = GAN_ADA()\n",
    "model.compile(\n",
    "    generator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    "    discriminator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    ")\n",
    "\n",
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"gan_model_ll\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5854,
     "status": "ok",
     "timestamp": 1676694863243,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "yvhv5De0CK03",
    "outputId": "0197c938-e635-41bd-a39d-804bb402f39b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f9bed5b2a00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights learned during training\n",
    "\n",
    "model.load_weights(\"DCGAN_Cifar_Plane_same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4100,
     "status": "ok",
     "timestamp": 1676694867322,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "blURqcoqDPl3",
    "outputId": "43fed292-c259-4b9f-fa46-696536d0c910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://tfhub.dev/captain-pool/esrgan-tf2/1, Total size: 20.60MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load ESRGAN model to increase the size of the generated images and improve their resolution \n",
    "\n",
    "os.environ[\"TFHUB_DOWNLOAD_PROGRESS\"] = \"True\"\n",
    "# Declaring Constants\n",
    "ESRGAN_path = \"https://tfhub.dev/captain-pool/esrgan-tf2/1\"\n",
    "model_super = hub.load(ESRGAN_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEcN6qC5Cbmh"
   },
   "outputs": [],
   "source": [
    "# Post Processing Functions to increase the size and resolution of the generated images\n",
    "\n",
    "def postprocess(image_tensor):\n",
    "\n",
    "    size = image_tensor.shape[1]\n",
    "    new_size = size * 2\n",
    "    # Upsample the image using TensorFlow\n",
    "    resized_image = tf.image.resize(image_tensor, [new_size, new_size], method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "    # Apply denoising using OpenCV\n",
    "    denoised_image = cv2.medianBlur(resized_image.numpy(), 3)\n",
    "\n",
    "    #Applying sharpening filter\n",
    "    pil_img = tf.keras.preprocessing.image.array_to_img(denoised_image)\n",
    "    sharp = pil_img.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    return sharp\n",
    "\n",
    "def resize_res(image_tensor):\n",
    "\n",
    "    # First denoise the image and prepare it for the ESRGAN model \n",
    "    denoised_image = cv2.medianBlur(image_tensor.numpy(), 3)\n",
    "    image = tf.expand_dims(denoised_image, 0)\n",
    "    image = image * 255\n",
    "\n",
    "    # Improve the image with the ESRGAN model\n",
    "    fake_image = model_super(image,training=False)\n",
    "\n",
    "    # Prepare image for plotting and later use\n",
    "    fake_image = tf.squeeze(fake_image)\n",
    "    fake_image = fake_image / 255\n",
    "\n",
    "    return fake_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkbKin4gCk-o"
   },
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "\n",
    "    # Declare necessary variables for plotting configuration\n",
    "    columns=5\n",
    "    width=25\n",
    "    height=10\n",
    "    max_images=15 \n",
    "\n",
    "    # Check if there are images for plotting\n",
    "    if not images:\n",
    "        print(\"No images to display.\")\n",
    "        return \n",
    "\n",
    "    # Plotting configuration    \n",
    "    height = max(height, int(len(images)/columns) * height)\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    # Plot the images in subplots\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "16_PEAUaG2LtnI4nO9SB0vxXjJw6bh6ax"
    },
    "executionInfo": {
     "elapsed": 93904,
     "status": "ok",
     "timestamp": 1676695042613,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "8onquXlICrDG",
    "outputId": "7eb09397-d248-4f18-80c3-5a076b32747f"
   },
   "outputs": [],
   "source": [
    "# Function to plot the best images generated from the model and their resized versions\n",
    "\n",
    "def plot_images_post(model, num_generated_images):\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    \n",
    "    # Generate a large number of images, to choose the best from\n",
    "    num_images = 5000\n",
    "    generated_images = model.generate(num_images, training=False)\n",
    "\n",
    "    # Extract the indices of the n best generated images based on the output from the discriminator\n",
    "    scores = model.discriminator(generated_images)\n",
    "    v, ind = tf.math.top_k(scores[:, 0], k=num_generated_images, sorted=True)\n",
    "    print(v)\n",
    "    print(ind)\n",
    "\n",
    "    # Iterate over the best images, apply the resizing and improvement functions and plot them all\n",
    "    for i in range(len(ind)):\n",
    "\n",
    "        # Extract the best images based on the indices\n",
    "        ii = ind[i]\n",
    "        image_tensor = generated_images[ii]\n",
    "\n",
    "        # First resizing and sharpening of the image (output size = 64*64*3)\n",
    "        sharp = postprocess(image_tensor)\n",
    "\n",
    "        # Second resizing and sharpening of the image (output size = 128*128*3)\n",
    "        sharp_array = tf.keras.preprocessing.image.img_to_array(sharp)\n",
    "        sharp1 = postprocess(sharp_array)\n",
    "\n",
    "        # Improvement of the resolution and resizing using the ESRGAN model (output size = 128*128*3)\n",
    "        res_image = resize_res(image_tensor)\n",
    "\n",
    "        # Second improvement of the resolution and resizing using the ESRGAN model (output size = 256*256*3)\n",
    "        resized_image_2 = tf.image.resize(image_tensor, [64, 64], method=tf.image.ResizeMethod.BICUBIC)\n",
    "        res_image_256 = resize_res(resized_image_2)\n",
    "\n",
    "        # Plotting of all images\n",
    "        imgs = [image_tensor,sharp,sharp1,res_image,res_image_256]\n",
    "        display_images(imgs)\n",
    "\n",
    "num_generated_images = 20        \n",
    "plot_images_post(model,num_generated_images)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 94665,
     "status": "ok",
     "timestamp": 1676684030708,
     "user": {
      "displayName": "Diego Ponce De León",
      "userId": "08071100372349839845"
     },
     "user_tz": -60
    },
    "id": "j2Ljd070UQGV",
    "outputId": "fa50397d-46c4-40d4-a409-ffeebc923c2b"
   },
   "outputs": [],
   "source": [
    "def plot_images_best(model, num_rows=5, num_cols=4):\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    \n",
    "    # Generate a large number of images, to choose the best from\n",
    "    num_images = 5000\n",
    "    generated_images = model.generate(num_images, training=False)\n",
    "\n",
    "    # Extract the indices of the n best generated images based on the output from the discriminator\n",
    "    scores = model.discriminator(generated_images)\n",
    "    num_gen = num_rows * num_cols\n",
    "    v, ind = tf.math.top_k(scores[:, 0], k=num_gen, sorted=False)\n",
    "    print(v)\n",
    "    print(ind)\n",
    "    \n",
    "    # Iterate over the best images and plot them all\n",
    "    plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            index = row * num_cols + col\n",
    "            ii = ind[index]\n",
    "            plt.subplot(num_rows, num_cols, index + 1)\n",
    "            plt.imshow(generated_images[ii])\n",
    "            \n",
    "            # Convert the tensor to a PIL Image\n",
    "            pil_img = tf.keras.preprocessing.image.array_to_img(generated_images[ii])\n",
    "\n",
    "            # Save the image to a file\n",
    "            pil_img.save(\"generated_images_cifar_plane_\"+str(index)+\".png\")\n",
    "            \n",
    "            plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()       \n",
    "        \n",
    "plot_images_best(model)        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPXNbFxxGZuqZY2js36/viH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
